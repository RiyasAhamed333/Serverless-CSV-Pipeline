ğŸš€ Serverless CSV Pipeline â€“ S3 â†’ Lambda â†’ DynamoDB
Flipkart Price Update Use Case | Automated Serverless Data Processing

ğŸ“Œ Project Overview

This project demonstrates a fully serverless, event-driven data pipeline built on AWS.
Whenever a CSV file is uploaded to Amazon S3, it automatically triggers a Lambda function that reads the file and performs:

INSERT

UPDATE

DELETE

operations on a DynamoDB table based on the action column in the CSV.

ğŸ”¥ Real-World Use Cases

âœ” Flipkart product price updates
âœ” Inventory synchronization
âœ” Bulk catalog import
âœ” Automated price corrections
âœ” E-commerce product ingestion

ğŸ—ï¸ Architecture Clean Visual Flow

| Flow | AWS Service          | Action                                                     |
| ---- | -------------------- | ---------------------------------------------------------- |
| ğŸ“„   | **CSV File**         | Data source uploaded by user                               |
| â¬‡ï¸   | **Amazon S3**        | CSV file is uploaded to S3 bucket                          |
| âš¡    | **S3 Event Trigger** | Automatically triggers Lambda on file upload               |
| ğŸ§    | **AWS Lambda**       | Parses CSV & performs Insert / Update / Delete in DynamoDB |
| ğŸ—„ï¸  | **Amazon DynamoDB**  | Stores updated product records                             |
| ğŸ“Š   | **CloudWatch Logs**  | Logs processing details, successes, and errors             |

ğŸ“‚ Folder Structure

serverless-csv-pipeline/
â”‚
â”œâ”€â”€ lambda_function.py      # Main Lambda code
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ sample.csv              # Example CSV file
â””â”€â”€ README.md               # Documentation


ğŸ§  How It Works

1ï¸âƒ£ Upload CSV to S3

Example CSV:
product_id,name,price,action
101,Redmi Note 12,15999,INSERT
102,Boat Headset,799,INSERT
101,Redmi Note 12,14999,UPDATE
102,Boat Headset,,DELETE

2ï¸âƒ£ S3 Triggers the Lambda Function

S3 Event Type:

s3:ObjectCreated:*


Lambda receives:

Bucket name
Object key

3ï¸âƒ£ Lambda Parses the CSV

Lambda checks the action column:

Action	Operation
INSERT	Add a new item
UPDATE	Update existing item
DELETE	Delete item from table

4ï¸âƒ£ DynamoDB is Updated

Data is written to a table containing fields like:

product_id (Partition Key)

name

price

And others based on CSV columns

5ï¸âƒ£ CloudWatch Logs

Every row action is logged:

â€œInserted: <product_id>â€

â€œUpdated: <product_id>â€

â€œDeleted: <product_id>â€

Error logs (if any)

ğŸ’» Lambda Function Code

(Upload the lambda_function.py provided earlier.)

ğŸ”§ Deployment Steps
1ï¸âƒ£ Create DynamoDB Table

Table Name: FlipkartProducts

Primary Key: product_id (String)

2ï¸âƒ£ Create an S3 Bucket

Example:

flipkart-product-csv-bucket

3ï¸âƒ£ Create Lambda Function

Runtime: Python 3.9+

Add environment variable:

DDB_TABLE = FlipkartProducts

4ï¸âƒ£ Add S3 Trigger

Navigate to:
S3 â†’ Bucket â†’ Properties â†’ Event Notifications

Configure:

Event type: PUT

Prefix: (optional)

Destination: Lambda Function

5ï¸âƒ£ IAM Permissions

Attach these managed policies to the Lambda execution role:

AmazonS3ReadOnlyAccess
AmazonDynamoDBFullAccess
CloudWatchLogsFullAccess

ğŸ“Š Sample CloudWatch Output

File received: s3://flipkart-bucket/sample.csv

Inserted: 101

Inserted: 102

Updated: 101

Deleted: 102

CSV processed successfully

ğŸ… Skills Demonstrated

AWS Lambda

Amazon S3

DynamoDB

IAM

CloudWatch Logs

Python CSV Processing

Serverless Architecture

Event-Driven Workflows
